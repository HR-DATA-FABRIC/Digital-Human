{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "15e5517a-94f3-4541-8560-9e1e2ea9416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart is a leading e-commerce company.\n",
      "[(0, 6), (8, 9), (11, 11), (13, 19), (21, 30), (32, 39)]\n",
      "(0, 7, 'ORG')\n",
      "I reached Chennai yesterday.\n",
      "[(0, 0), (2, 8), (10, 16), (18, 27)]\n",
      "(18, 28, 'GPE')\n",
      "I recently ordered a book from Amazon.\n",
      "[(0, 0), (2, 9), (11, 17), (19, 19), (21, 24), (26, 29), (31, 37)]\n",
      "(31, 38, 'ORG')\n",
      "I was driving a BMW.\n",
      "[(0, 0), (2, 4), (6, 12), (14, 14), (16, 19)]\n",
      "(16, 20, 'PRODUCT')\n",
      "I ordered this from ShopClues.\n",
      "[(0, 0), (2, 8), (10, 13), (15, 18), (20, 29)]\n",
      "(20, 30, 'ORG')\n",
      "Fridge can be ordered in Amazon.\n",
      "[(0, 5), (7, 9), (11, 12), (14, 20), (22, 23), (25, 31)]\n",
      "(0, 6, 'PRODUCT')\n",
      "I bought a new Washer.\n",
      "[(0, 0), (2, 7), (9, 9), (11, 13), (15, 21)]\n",
      "(15, 21, 'PRODUCT')\n",
      "I bought a old table.\n",
      "[(0, 0), (2, 7), (9, 9), (11, 13), (15, 20)]\n",
      "(15, 20, 'PRODUCT')\n",
      "I bought a fancy dress.\n",
      "[(0, 0), (2, 7), (9, 9), (11, 15), (17, 22)]\n",
      "(17, 23, 'PRODUCT')\n",
      "I rented a camera.\n",
      "[(0, 0), (2, 7), (9, 9), (11, 17)]\n",
      "(11, 18, 'PRODUCT')\n",
      "I rented a tent for our trip.\n",
      "[(0, 0), (2, 7), (9, 9), (11, 14), (16, 18), (20, 22), (24, 28)]\n",
      "(11, 15, 'PRODUCT')\n",
      "I rented a screwdriver from our neighbour.\n",
      "[(0, 0), (2, 7), (9, 9), (11, 21), (23, 26), (28, 30), (32, 41)]\n",
      "(11, 22, 'PRODUCT')\n",
      "I repaired my computer.\n",
      "[(0, 0), (2, 9), (11, 12), (14, 22)]\n",
      "(14, 23, 'PRODUCT')\n",
      "I got my clock fixed.\n",
      "[(0, 0), (2, 4), (6, 7), (9, 13), (15, 20)]\n",
      "(9, 14, 'PRODUCT')\n",
      "I got my truck fixed.\n",
      "[(0, 0), (2, 4), (6, 7), (9, 13), (15, 20)]\n",
      "(9, 14, 'PRODUCT')\n",
      "Flipkart started it's journey from zero.\n",
      "[(0, 7), (9, 15), (17, 20), (22, 28), (30, 33), (35, 39)]\n",
      "(0, 8, 'ORG')\n",
      "I recently ordered from Max.\n",
      "[(0, 0), (2, 9), (11, 17), (19, 22), (24, 27)]\n",
      "(24, 28, 'ORG')\n",
      "Flipkart is recognized as leader in market.\n",
      "[(0, 7), (9, 10), (12, 21), (23, 24), (26, 31), (33, 34), (36, 42)]\n",
      "(0, 8, 'ORG')\n",
      "I recently ordered from Swiggy.\n",
      "[(0, 0), (2, 9), (11, 17), (19, 22), (24, 30)]\n",
      "(24, 31, 'ORG')\n"
     ]
    }
   ],
   "source": [
    "# =====> https://realpython.com/natural-language-processing-spacy-python/\n",
    "\n",
    "# TEST FOR MISALIGNMENT\n",
    "\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company.\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(18, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon.\", {\"entities\": [(31,38, \"ORG\")]}),\n",
    "              (\"I was driving a BMW.\", {\"entities\": [(16,20, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues.\", {\"entities\": [(20,30, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon.\", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer.\", {\"entities\": [(15,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table.\", {\"entities\": [(15,20, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress.\", {\"entities\": [(17,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera.\", {\"entities\": [(11,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip.\", {\"entities\": [(11,15, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour.\", {\"entities\": [(11,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer.\", {\"entities\": [(14,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed.\", {\"entities\": [(9,14, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed.\", {\"entities\": [(9,14, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero.\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max.\", {\"entities\": [(24,28, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market.\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy.\", {\"entities\": [(24,31, \"ORG\")]})\n",
    "              ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "spacy.require_gpu()\n",
    "from spacy.training import offsets_to_biluo_tags  \n",
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "\n",
    "for text, annotations, in TRAIN_DATA:\n",
    "    text = nlp.make_doc(text)\n",
    "    print(text)\n",
    "    print([(ele.start(), ele.end() - 1) for ele in re.finditer(r'\\S+', str(text))])\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "        print(ent)    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "27a9a5ee-7f02-4988-bb37-23a6e4ecd699",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company.\", [(0, 7, \"ORG\")]),\n",
    "              (\"I reached Chennai yesterday.\", [(18, 28, \"GPE\")]),\n",
    "              (\"I recently ordered a book from Amazon.\", [(31,38, \"ORG\")]),\n",
    "              (\"I was driving a BMW.\", [(16,20, \"PRODUCT\")]),\n",
    "              (\"I ordered this from ShopClues.\", [(20,30, \"ORG\")]),\n",
    "              (\"Fridge can be ordered in Amazon.\", [(0,16, \"PRODUCT\")]),\n",
    "              (\"I bought a new Washer.\", [(15,21, \"PRODUCT\")]),\n",
    "              (\"I bought a old table.\", [(15,20, \"PRODUCT\")]),\n",
    "              (\"I bought a fancy dress.\", [(17,23, \"PRODUCT\")]),\n",
    "              (\"I rented a camera.\", [(11,18, \"PRODUCT\")]),\n",
    "              (\"I rented a tent for our trip.\", [(11,15, \"PRODUCT\")]),\n",
    "              (\"I rented a screwdriver from our neighbour.\", [(11,22, \"PRODUCT\")]),\n",
    "              (\"I repaired my computer.\", [(14,23, \"PRODUCT\")]),\n",
    "              (\"I got my clock fixed.\", [(9,14, \"PRODUCT\")]),\n",
    "              (\"I got my truck fixed.\", [(9,14, \"PRODUCT\")]),\n",
    "              (\"Flipkart started it's journey from zero.\", [(0,8, \"ORG\")]),\n",
    "              (\"I recently ordered from Max.\", [(24,28, \"ORG\")]),\n",
    "              (\"Flipkart is recognized as leader in market.\",[(0,8, \"ORG\")]),\n",
    "              (\"I recently ordered from Swiggy.\", [(24,31, \"ORG\")]),\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "876ac608-22af-44b0-afeb-e76887a7770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart\n",
      "()\n",
      "yesterday.\n",
      "(yesterday,)\n",
      "Amazon.\n",
      "(Amazon,)\n",
      "BMW.\n",
      "(BMW,)\n",
      "ShopClues.\n",
      "(ShopClues,)\n",
      "None\n",
      "(Amazon,)\n",
      "Washer\n",
      "()\n",
      "table\n",
      "()\n",
      "dress.\n",
      "()\n",
      "camera.\n",
      "()\n",
      "tent\n",
      "()\n",
      "screwdriver\n",
      "()\n",
      "computer.\n",
      "()\n",
      "clock\n",
      "()\n",
      "truck\n",
      "()\n",
      "Flipkart\n",
      "(Flipkart, zero)\n",
      "Max.\n",
      "(Max,)\n",
      "Flipkart\n",
      "(Flipkart,)\n",
      "Swiggy.\n",
      "(Swiggy,)\n"
     ]
    }
   ],
   "source": [
    "# ====> TESTING for faulty annotations\n",
    "# https://spacy.io/api/top-level#offsets_to_biluo_tags\n",
    "from spacy.training import offsets_to_biluo_tags  \n",
    "\n",
    "for text, annotations in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "        span = doc.char_span(start, end, label = label)\n",
    "        ents.append(span)\n",
    "        print(span)\n",
    "        print(doc.ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a294d579-7b62-4142-9395-dfba30a000e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', '-', '-', 'O']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "doc = nlp(\"I like daat London.\")\n",
    "entities = [(7, 13, \"LOC\")]\n",
    "tags = offsets_to_biluo_tags(doc, entities)\n",
    "print(tags)\n",
    "\n",
    "#assert tags == [\"O\", \"O\", \"U-LOC\", \"U-LOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "09d5c34a-e600-41e1-9220-7e8022621c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREPROCESS.PY\n",
    "#### PREPROCESS.PY\n",
    "#### PREPROCESS.PY\n",
    "#### PREPROCESS.PY\n",
    "#### PREPROCESS.PY\n",
    "#### NER ACCORDING to SCPAY MANUAL 3\n",
    "#### NER ACCORDING to SCPAY MANUAL 3\n",
    "#### NER ACCORDING to SCPAY MANUAL 3\n",
    "#### NER ACCORDING to SCPAY MANUAL 3\n",
    "#### NER ACCORDING to SCPAY MANUAL 3\n",
    "# ===> https://spacy.io/usage/training\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "training_data = [\n",
    "            (\"Walmart is a leading e-commerce company.\", [(0, 7, \"ORG\")]),\n",
    "            (\"I reached Chennai yesterday.\", [(18, 28, \"GPE\")]),\n",
    "            (\"I recently ordered a book from Amazon.\", [(31,38, \"ORG\")]),\n",
    "            (\"I was driving a BMW.\", [(16,20, \"PRODUCT\")]),\n",
    "            (\"I ordered this from ShopClues.\", [(20,30, \"ORG\")]),\n",
    "            (\"Fridge can be ordered in Amazon.\", [(0,6, \"PRODUCT\")]),\n",
    "            (\"I bought a new Washer.\", [(15,21, \"PRODUCT\")]),\n",
    "            (\"I bought a old table.\", [(15,20, \"PRODUCT\")]),\n",
    "            (\"I bought a fancy dress.\", [(17,23, \"PRODUCT\")]),\n",
    "            (\"I rented a camera.\", [(11,18, \"PRODUCT\")]),\n",
    "            (\"I rented a tent for our trip.\", [(11,15, \"PRODUCT\")]),\n",
    "            (\"I rented a screwdriver from our neighbour.\", [(11,22, \"PRODUCT\")]),\n",
    "            (\"I repaired my computer.\", [(14,23, \"PRODUCT\")]),\n",
    "            (\"I got my clock fixed.\", [(9,14, \"PRODUCT\")]),\n",
    "            (\"Flipkart started it's journey from zero.\", [(0,8, \"ORG\")]),\n",
    "            (\"I recently ordered from Max.\", [(24,28, \"ORG\")]),\n",
    "            (\"Flipkart is recognized as leader in market.\",[(0,8, \"ORG\")]),\n",
    "            (\"I recently ordered from Swiggy.\", [(24,31, \"ORG\")]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "df948c45-69be-4dd9-9db2-a9c846c54f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7, 'ORG')] ['U-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[(18, 28, 'GPE')] ['O', 'O', 'O', 'B-GPE', 'L-GPE']\n",
      "[(31, 38, 'ORG')] ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'L-ORG']\n",
      "[(16, 20, 'PRODUCT')] ['O', 'O', 'O', 'O', 'B-PRODUCT', 'L-PRODUCT']\n",
      "[(20, 30, 'ORG')] ['O', 'O', 'O', 'O', 'B-ORG', 'L-ORG']\n",
      "[(0, 6, 'PRODUCT')] ['U-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[(15, 21, 'PRODUCT')] ['O', 'O', 'O', 'O', 'U-PRODUCT', 'O']\n",
      "[(15, 20, 'PRODUCT')] ['O', 'O', 'O', 'O', 'U-PRODUCT', 'O']\n",
      "[(17, 23, 'PRODUCT')] ['O', 'O', 'O', 'O', 'B-PRODUCT', 'L-PRODUCT']\n",
      "[(11, 18, 'PRODUCT')] ['O', 'O', 'O', 'B-PRODUCT', 'L-PRODUCT']\n",
      "[(11, 15, 'PRODUCT')] ['O', 'O', 'O', 'U-PRODUCT', 'O', 'O', 'O', 'O']\n",
      "[(11, 22, 'PRODUCT')] ['O', 'O', 'O', 'U-PRODUCT', 'O', 'O', 'O', 'O']\n",
      "[(14, 23, 'PRODUCT')] ['O', 'O', 'O', 'B-PRODUCT', 'L-PRODUCT']\n",
      "[(9, 14, 'PRODUCT')] ['O', 'O', 'O', 'U-PRODUCT', 'O', 'O']\n",
      "[(0, 8, 'ORG')] ['U-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[(24, 28, 'ORG')] ['O', 'O', 'O', 'O', 'B-ORG', 'L-ORG']\n",
      "[(0, 8, 'ORG')] ['U-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[(24, 31, 'ORG')] ['O', 'O', 'O', 'O', 'B-ORG', 'L-ORG']\n"
     ]
    }
   ],
   "source": [
    "# TEST for correct ANNOTATION offsets \n",
    "#### '-' MEANS wrong start or end offset\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "for text, annotations in training_data:\n",
    "    doc = nlp(text)\n",
    "    tags = offsets_to_biluo_tags(doc, annotations)\n",
    "    print(annotations, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6c7a7939-c31b-49ee-9cb5-348206492048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart is a leading e-commerce company. ORG [Walmart]\n",
      "I reached Chennai yesterday. GPE [yesterday.]\n",
      "I recently ordered a book from Amazon. ORG [Amazon.]\n",
      "I was driving a BMW. PRODUCT [BMW.]\n",
      "I ordered this from ShopClues. ORG [ShopClues.]\n",
      "Fridge can be ordered in Amazon. PRODUCT [Fridge]\n",
      "I bought a new Washer. PRODUCT [Washer]\n",
      "I bought a old table. PRODUCT [table]\n",
      "I bought a fancy dress. PRODUCT [dress.]\n",
      "I rented a camera. PRODUCT [camera.]\n",
      "I rented a tent for our trip. PRODUCT [tent]\n",
      "I rented a screwdriver from our neighbour. PRODUCT [screwdriver]\n",
      "I repaired my computer. PRODUCT [computer.]\n",
      "I got my clock fixed. PRODUCT [clock]\n",
      "Flipkart started it's journey from zero. ORG [Flipkart]\n",
      "I recently ordered from Max. ORG [Max.]\n",
      "Flipkart is recognized as leader in market. ORG [Flipkart]\n",
      "I recently ordered from Swiggy. ORG [Swiggy.]\n"
     ]
    }
   ],
   "source": [
    "# the DocBin will store the example documentsdb = DocBin()\n",
    "# ===> text voor correct ENTITY anntotation\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "db = DocBin().from_disk(\"train.spacy\")\n",
    "for doc in db.get_docs(nlp.vocab):\n",
    "    assert doc.has_annotation(\"TAG\")\n",
    "    # or just inspect the tags\n",
    "    for token in doc:\n",
    "        print(token.text, token.tag_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for text, annotations in training_data:\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    print(doc, label, ents)\n",
    "    db.add(doc)\n",
    "db.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "41c90f53-0c82-493e-ab5f-0f20aaef4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> https://new-languages-for-nlp.github.io/course-materials/w2/practical-intro/Practical%20Introduction%20to%20Model%20Training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82a2906d-c6b0-45c5-803e-ab1aec640974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart is a leading e-commerce company.\n",
      "I reached Chennai yesterday.\n",
      "I recently ordered a book from Amazon.\n",
      "I was driving a BMW.\n",
      "I ordered this from ShopClues.\n",
      "Fridge can be ordered in Amazon.\n",
      "I bought a new Washer.\n",
      "I bought a old table.\n",
      "I bought a fancy dress.\n",
      "I rented a camera.\n",
      "I rented a tent for our trip.\n",
      "I rented a screwdriver from our neighbour.\n",
      "I repaired my computer.\n",
      "I got my clock fixed.\n",
      "Flipkart started it's journey from zero.\n",
      "I recently ordered from Max.\n",
      "Flipkart is recognized as leader in market.\n",
      "I recently ordered from Swiggy.\n"
     ]
    }
   ],
   "source": [
    "### RED ONLY TEXT form training_data\n",
    "\n",
    "for text, ent in training_data:\n",
    "    doc = nlp(text)\n",
    "    #print(doc, ent)\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0f3a5a12-db80-418a-9a58-e74dbcd1cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### spaCy can recognize various types of named entities in a document, by asking the model for a prediction. \n",
    "#### NOTE it does not recogninzse WALMART is a leading e-commerce company\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Walmart is a leading e-commerce company.\")\n",
    "#doc = nlp(\"Walmart is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bdcab-81cb-434f-bd78-130fb45d780f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce14b72-573f-4dd9-864d-b63d95928783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea08b97-94fe-4ed2-aeda-265553181d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
