{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "77c15527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'component', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Compute preexcisting SpaCy NER model\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "from spacy.language import Language\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Correct for more precise bounaries\n",
    "boundary = re.compile('^[0-9]$')\n",
    "\n",
    "@Language.component(\"component\")\n",
    "def custom_seg(doc):\n",
    "    prev = doc[0].text\n",
    "    length = len(doc)\n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.text == '.' and boundary.match(prev) and index!=(length - 1)):\n",
    "            doc[index+1].sent_start = False\n",
    "        prev = token.text\n",
    "    return doc\n",
    "    \n",
    "nlp.add_pipe(\"component\", before='parser')\n",
    "\n",
    "# Getting the pipeline NER component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77c0294a-b29b-4f46-b710-326c5685b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training custom NER annotated data\n",
    "\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]\n",
    "\n",
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "  for ent in annotations.get(\"entities\"):\n",
    "    ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82b99738-3031-45ca-afe4-d29cf4d147f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10:\n",
      "{'ner': 42.39633257508103}\n",
      "Epoch 2 of 10:\n",
      "{'ner': 10.42556143421367}\n",
      "Epoch 3 of 10:\n",
      "{'ner': 24.5927018098386}\n",
      "Epoch 4 of 10:\n",
      "{'ner': 5.70484348848452}\n",
      "Epoch 5 of 10:\n",
      "{'ner': 3.607417439913781}\n",
      "Epoch 6 of 10:\n",
      "{'ner': 1.483973307798065}\n",
      "Epoch 7 of 10:\n",
      "{'ner': 1.9861611010679505}\n",
      "Epoch 8 of 10:\n",
      "{'ner': 0.2707683855519303}\n",
      "Epoch 9 of 10:\n",
      "{'ner': 0.002296802585233395}\n",
      "Epoch 10 of 10:\n",
      "{'ner': 4.471921552840087e-06}\n"
     ]
    }
   ],
   "source": [
    "# Training a extended NER model\n",
    "# Disable pipeline components you don't need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "nlp = spacy.blank('en')\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for label in annotations['entities']:\n",
    "        ner.add_label(label[2])\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "from spacy.training import Example        \n",
    "        \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(10):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        print(f'Epoch {epoch+1} of {10}:')\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.2, sgd=optimizer, losses=losses) #SGD\n",
    "        print(losses) #Print losses after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1f45ef7-d38c-461b-b161-d93de10f2784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Alto', 'PRODUCT')]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "247716ce-14cc-4d92-b35a-946e81b54c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to directory model\n"
     ]
    }
   ],
   "source": [
    "# Save the  model to directory\n",
    "from pathlib import Path\n",
    "output_dir = Path('./model/')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to directory\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cfd87492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from model directory model\n",
      "Entities [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and predict\n",
    "print(\"Loading from model directory\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef7b60db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['component', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Train NER from a blank spacy model\n",
    "import spacy\n",
    "import re\n",
    "from spacy.language import Language\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "# Correct for more precise bounaries\n",
    "boundary = re.compile('^[0-9]$')\n",
    "\n",
    "@Language.component(\"component\")\n",
    "def custom_seg(doc):\n",
    "    prev = doc[0].text\n",
    "    length = len(doc)\n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.text == '.' and boundary.match(prev) and index!=(length - 1)):\n",
    "            doc[index+1].sent_start = False\n",
    "        prev = token.text\n",
    "    return doc\n",
    "    \n",
    "nlp.add_pipe(\"component\")\n",
    "\n",
    "ner = nlp.add_pipe('ner')\n",
    "nlp.begin_training()\n",
    "\n",
    "print(nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a25f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e9f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
