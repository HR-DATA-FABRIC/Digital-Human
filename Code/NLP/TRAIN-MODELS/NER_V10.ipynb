{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac90ada",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83d1aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'component', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Compute pre-excisting SpaCy NER model\n",
    "\n",
    "import spacy\n",
    "import re \n",
    "from spacy.language import Language\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Correct for more precise bounaries\n",
    "boundary = re.compile('^[0-9]$')\n",
    "\n",
    "@Language.component(\"component\")\n",
    "def custom_seg(doc):\n",
    "    prev = doc[0].text\n",
    "    length = len(doc)\n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.text == '.' and boundary.match(prev) and index!=(length - 1)):\n",
    "            doc[index+1].sent_start = False\n",
    "        prev = token.text\n",
    "    return doc\n",
    "    \n",
    "nlp.add_pipe(\"component\", before='parser')\n",
    "\n",
    "# Getting the pipeline NER component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c0294a-b29b-4f46-b710-326c5685b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training custom NER annotated data\n",
    "\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]\n",
    "\n",
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "  for ent in annotations.get(\"entities\"):\n",
    "    ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b99738-3031-45ca-afe4-d29cf4d147f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10:\n",
      "{'ner': 48.336134323239094}\n",
      "Epoch 2 of 10:\n",
      "{'ner': 12.61607434726426}\n",
      "Epoch 3 of 10:\n",
      "{'ner': 16.003298977755666}\n",
      "Epoch 4 of 10:\n",
      "{'ner': 4.932249405850584}\n",
      "Epoch 5 of 10:\n",
      "{'ner': 3.137065685928928}\n",
      "Epoch 6 of 10:\n",
      "{'ner': 1.8671225861252847}\n",
      "Epoch 7 of 10:\n",
      "{'ner': 1.1325932176187525}\n",
      "Epoch 8 of 10:\n",
      "{'ner': 0.08740469777323746}\n",
      "Epoch 9 of 10:\n",
      "{'ner': 0.0004492976952553009}\n",
      "Epoch 10 of 10:\n",
      "{'ner': 0.027222024830030427}\n"
     ]
    }
   ],
   "source": [
    "# Training a extended NER model\n",
    "# Disable pipeline components you don't need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "nlp = spacy.blank('en')\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for label in annotations['entities']:\n",
    "        ner.add_label(label[2])\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "from spacy.training import Example        \n",
    "        \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(10):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        print(f'Epoch {epoch+1} of {10}:')\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.2, sgd=optimizer, losses=losses) #SGD\n",
    "        print(losses) #Print losses after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f45ef7-d38c-461b-b161-d93de10f2784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247716ce-14cc-4d92-b35a-946e81b54c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to directory model\n"
     ]
    }
   ],
   "source": [
    "# Save the  model to directory\n",
    "from pathlib import Path\n",
    "output_dir = Path('./model/')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to directory\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c28cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from model directory model\n",
      "Entities [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and predict\n",
    "print(\"Loading from model directory\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ea4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['component', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE to train NER from a blank spacy model\n",
    "import spacy\n",
    "import re\n",
    "from spacy.language import Language\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "# Correct for more precise bounaries\n",
    "boundary = re.compile('^[0-9]$')\n",
    "\n",
    "@Language.component(\"component\")\n",
    "def custom_seg(doc):\n",
    "    prev = doc[0].text\n",
    "    length = len(doc)\n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.text == '.' and boundary.match(prev) and index!=(length - 1)):\n",
    "            doc[index+1].sent_start = False\n",
    "        prev = token.text\n",
    "    return doc\n",
    "    \n",
    "nlp.add_pipe(\"component\")\n",
    "\n",
    "ner = nlp.add_pipe('ner')\n",
    "nlp.begin_training()\n",
    "\n",
    "print(nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c73d5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'component', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE Training completely new entity type in spaCy\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "from spacy.language import Language\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Correct for more precise bounaries\n",
    "boundary = re.compile('^[0-9]$')\n",
    "\n",
    "@Language.component(\"component\")\n",
    "def custom_seg(doc):\n",
    "    prev = doc[0].text\n",
    "    length = len(doc)\n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.text == '.' and boundary.match(prev) and index!=(length - 1)):\n",
    "            doc[index+1].sent_start = False\n",
    "        prev = token.text\n",
    "    return doc\n",
    "    \n",
    "nlp.add_pipe(\"component\", before='parser')\n",
    "\n",
    "# Getting the pipeline NER component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700e24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3a26c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c07faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30:\n",
      "{'ner': 53.49527291953564}\n",
      "Epoch 2 of 30:\n",
      "{'ner': 19.49522678235462}\n",
      "Epoch 3 of 30:\n",
      "{'ner': 11.43590301005572}\n",
      "Epoch 4 of 30:\n",
      "{'ner': 7.849987847871769}\n",
      "Epoch 5 of 30:\n",
      "{'ner': 3.7057412351612635}\n",
      "Epoch 6 of 30:\n",
      "{'ner': 3.625667381263947}\n",
      "Epoch 7 of 30:\n",
      "{'ner': 3.6103162439353977}\n",
      "Epoch 8 of 30:\n",
      "{'ner': 3.789772008193676}\n",
      "Epoch 9 of 30:\n",
      "{'ner': 2.12324987203493}\n",
      "Epoch 10 of 30:\n",
      "{'ner': 1.8972006927803966}\n",
      "Epoch 11 of 30:\n",
      "{'ner': 10.62688447760528}\n",
      "Epoch 12 of 30:\n",
      "{'ner': 2.904349904697667}\n",
      "Epoch 13 of 30:\n",
      "{'ner': 2.3366008946840586}\n",
      "Epoch 14 of 30:\n",
      "{'ner': 11.29514894845379}\n",
      "Epoch 15 of 30:\n",
      "{'ner': 1.396029188028388}\n",
      "Epoch 16 of 30:\n",
      "{'ner': 0.9264486979814582}\n",
      "Epoch 17 of 30:\n",
      "{'ner': 0.9526966321408676}\n",
      "Epoch 18 of 30:\n",
      "{'ner': 0.04170757135520367}\n",
      "Epoch 19 of 30:\n",
      "{'ner': 0.019767511927469346}\n",
      "Epoch 20 of 30:\n",
      "{'ner': 0.00032466260415628644}\n",
      "Epoch 21 of 30:\n",
      "{'ner': 0.3879618703161943}\n",
      "Epoch 22 of 30:\n",
      "{'ner': 0.00021439766538737927}\n",
      "Epoch 23 of 30:\n",
      "{'ner': 6.986041051827311e-05}\n",
      "Epoch 24 of 30:\n",
      "{'ner': 0.007977056054094529}\n",
      "Epoch 25 of 30:\n",
      "{'ner': 0.050901079199193906}\n",
      "Epoch 26 of 30:\n",
      "{'ner': 0.000280540885270737}\n",
      "Epoch 27 of 30:\n",
      "{'ner': 1.0593677536396342e-05}\n",
      "Epoch 28 of 30:\n",
      "{'ner': 2.8866013537922182e-06}\n",
      "Epoch 29 of 30:\n",
      "{'ner': 5.527567486776308e-07}\n",
      "Epoch 30 of 30:\n",
      "{'ner': 3.6200387976429803e-06}\n"
     ]
    }
   ],
   "source": [
    "# Training a extended NER model\n",
    "# Disable pipeline components you don't need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "nlp = spacy.blank('en')\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for label in annotations['entities']:\n",
    "        ner.add_label(label[2])\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "from spacy.training import Example        \n",
    "        \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(30):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        print(f'Epoch {epoch+1} of {30}:')\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.35, sgd=optimizer, losses=losses) #SGD\n",
    "        print(losses) #Print losses after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96255a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "Sushi\n",
      "Maggi\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent)\n",
    "#> Entities in 'I ate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "037393e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Pizza', 'FOOD')]\n",
      "Tokens [('Pizza', 'FOOD', 3), ('is', '', 2), ('a', '', 2), ('common', '', 2), ('fast', '', 2), ('food', '', 2), ('.', '', 2)]\n",
      "Entities [('Unagi', 'FOOD')]\n",
      "Tokens [('Unagi', 'FOOD', 3), ('is', '', 2), ('a', '', 2), ('famous', '', 2), ('seafood', '', 2), ('of', '', 2), ('Japan', '', 2)]\n",
      "Entities [('Tempura', 'FOOD')]\n",
      "Tokens [('Tempura', 'FOOD', 3), (',', '', 2), ('Soba', '', 2), ('are', '', 2), ('other', '', 2), ('famous', '', 2), ('dishes', '', 2), ('of', '', 2), ('Japan', '', 2)]\n",
      "Entities [('Lasagna', 'FOOD')]\n",
      "Tokens [('Lasagna', 'FOOD', 3), ('is', '', 2), ('another', '', 2), ('classic', '', 2), ('of', '', 2), ('Italy', '', 2)]\n",
      "Entities [('Frenchfries', 'FOOD')]\n",
      "Tokens [('Frenchfries', 'FOOD', 3), ('are', '', 2), ('considered', '', 2), ('too', '', 2), ('oily', '', 2)]\n",
      "Entities []\n",
      "Tokens [('China', '', 2), (\"'s\", '', 2), ('noodles', '', 2), ('are', '', 2), ('very', '', 2), ('famous', '', 2)]\n",
      "Entities [('Chocolate soufflé', 'FOOD')]\n",
      "Tokens [('Chocolate', 'FOOD', 3), ('soufflé', 'FOOD', 1), ('is', '', 2), ('extremely', '', 2), ('famous', '', 2), ('french', '', 2), ('cuisine', '', 2)]\n",
      "Entities [('Udon', 'ORG')]\n",
      "Tokens [('Udon', 'ORG', 3), ('is', '', 2), ('a', '', 2), ('healthy', '', 2), ('type', '', 2), ('of', '', 2), ('noodles', '', 2)]\n",
      "Entities [('Flamiche', 'FOOD')]\n",
      "Tokens [('Flamiche', 'FOOD', 3), ('is', '', 2), ('french', '', 2), ('pastry', '', 2)]\n",
      "Entities [('Shrimps', 'FOOD')]\n",
      "Tokens [('Shrimps', 'FOOD', 3), ('are', '', 2), ('famous', '', 2), ('in', '', 2), ('China', '', 2), ('too', '', 2)]\n",
      "Entities [('Burgers', 'FOOD')]\n",
      "Tokens [('Burgers', 'FOOD', 3), ('are', '', 2), ('the', '', 2), ('most', '', 2), ('commonly', '', 2), ('consumed', '', 2), ('fastfood', '', 2)]\n",
      "Entities [('Burgers', 'FOOD')]\n",
      "Tokens [('Burgers', 'FOOD', 3), ('are', '', 2), ('the', '', 2), ('most', '', 2), ('commonly', '', 2), ('consumed', '', 2), ('fastfood', '', 2)]\n",
      "Entities [('Pasta', 'FOOD')]\n",
      "Tokens [('Pasta', 'FOOD', 3), ('is', '', 2), ('an', '', 2), ('italian', '', 2), ('recipe', '', 2)]\n",
      "Entities [('Sushi', 'FOOD')]\n",
      "Tokens [('Sushi', 'FOOD', 3), ('is', '', 2), ('extemely', '', 2), ('famous', '', 2), ('and', '', 2), ('expensive', '', 2), ('Japanese', '', 2), ('dish', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c11437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "from pathlib import Path\n",
    "output_dir=Path('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc9a983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to folder:  model\n"
     ]
    }
   ],
   "source": [
    "# Saving the model to the output directory\n",
    "if not output_dir.exists():\n",
    "  output_dir.mkdir()\n",
    "nlp.meta['name'] = 'my_ner'  # rename model\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to folder: \", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d81fb36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from folder: model\n"
     ]
    }
   ],
   "source": [
    "# Loading the model from the directory\n",
    "print(\"Loading from folder:\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caed3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG Dosa\n"
     ]
    }
   ],
   "source": [
    "###assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "doc2 = nlp2(' Dosa is an extremely famous south Indian dish')\n",
    "for ent in doc2.ents:\n",
    "  print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73498c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
